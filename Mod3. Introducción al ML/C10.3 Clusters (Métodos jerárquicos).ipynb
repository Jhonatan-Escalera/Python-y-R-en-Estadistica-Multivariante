{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos jerárquicos\n",
    "\n",
    "Los métodos jerárquicos parten de una matriz de distancias o similaridades entre los elementos de la muestra y construyen una jerarquía basada en las distancias. \n",
    "\n",
    "Hay dos tipos de métodos jerárquicos para Clusterización:\n",
    "\n",
    "1. **Aglomerativos.** empezamos con $n$ (el número de observaciones) clústeres y los vamos agrupando en menos cantidad de clústeres que serán de mayor tamaño porque los iniciales solo contienen una observación.\n",
    "\n",
    "2. **Divisivos.** empezamos con un solo clúster que contiene a todas las observaciones, y vamos dividiendo en clústeres más pequeños.\n",
    "\n",
    "Los resultados dependen de las distancias consideradas. En particular, si los datos tienen diferentes unidades de medida, y la distancia usada no tiene en cuenta esto, es mejor estandarizar las variables.\n",
    "\n",
    "Para decidir si estandarizar las variables o no antes del análisis conviene tener en cuenta el objetivo del estudio. Si no estandarizamos, la distancia euclídea dependerá sobre todo de las variables con valores más grandes, y el resultado del análisis puede cambiar completamente al modificar su escala de medida. Si estandarizamos, estamos dando a priori un peso semejante a las variables, con independencia de su variabilidad original, lo que puede no ser siempre adecuado.\n",
    "\n",
    "Al final del análisis jerárquico, nos encontraremos con un gráfico llamado dendograma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método aglomerativo (agnes)\n",
    "\n",
    "Siempre tienen la misma estructura y sólo se diferencian en la forma de calcular las distancias entre grupos. Su estructura es:\n",
    "\n",
    "1. Comenzar con tantas clases como elementos, $n$. Las distancias entre clases son las distancias entre los elementos originales.\n",
    "\n",
    "\n",
    "2. Seleccionar los dos elementos más próximos en la matriz de distancias y formar con ellos una clase.\n",
    "\n",
    "\n",
    "3. Sustituir los dos elementos utilizados en el punto (2) para definir la clase, por un nuevo elemento que represente la clase construida. Las distancias entre este nuevo elemento y los anteriores se calculan con uno de los criterios para definir distancias entre grupos que comentaremos luego.\n",
    "\n",
    "\n",
    "4. Volver a (2), y repetir (2) y (3) hasta que tengamos todos los elementos agrupados en una clase única."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterios para decidir las distancias entre grupos\n",
    "\n",
    "Supongamos que tenemos un grupo $A$ con $n_A$ elementos, y un grupo $B$ con $n_B$ elementos, y que ambos se fusionan para crear un grupo $AB$ con $n_A+n_B$ elementos.\n",
    "\n",
    "La distancia del nuevo grupo, $AB$, a otro grupo $C$ con $n_C$ elementos, se calcula habitualmente por alguna de las cinco reglas siguientes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Single linkage.** Encadenamiento simple o vecino más próximo: La distancia entre los dos nuevos grupos es la menor de las distancias entre grupos antes de la fusión. Es decir:\n",
    "$$d(C,AB)=\\min\\{d(C,A),d(C,B)\\}$$\n",
    "\n",
    "Este criterio es invariante ante transformaciones monótonas. Tiende a producir grupos alargados.\n",
    "\n",
    "<img src=\"dis_grupos_11.png\">\n",
    "\n",
    "<img src=\"dis_grupos_12.png\">\n",
    "\n",
    "<img src=\"dendograma_single.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Complete linkage.** Encadenamiento completo o vecino más alejado: La distancia entre los dos nuevos grupos es la mayor de las distancias entre grupos antes de la fusión. Es decir:\n",
    "$$d(C,AB)=\\max\\{d(C,A),d(C,B)\\}$$\n",
    "\n",
    "Este criterio es invariante ante transformaciones monótonas. Tiende a producir grupos esféricos.\n",
    "\n",
    "<img src=\"dis_grupos_11.png\">\n",
    "\n",
    "<img src=\"dis_grupos_22.png\">\n",
    "\n",
    "<img src=\"dendograma_complete.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Average linkage) Media de grupos.** La distancia entre los dos nuevos grupos es la media ponderada entre las distancias entre grupos antes de la fusión:\n",
    "$$d(C,AB)=\\frac{n_A}{n_A+n_B}d(C,A)+\\frac{n_B}{n_A+n_B}d(C,B)$$\n",
    "\n",
    "Este criterio no es invariante ante transformaciones monótonas de las distancias.\n",
    "\n",
    "\n",
    "4. **(Centroid linkage) Método del centroide.** Se aplica generalmente sólo con variables continuas. La distancia entre dos grupos se hace igual a la distancia euclídea entre sus centros, donde se toman como centros los vectores de medias de las observaciones que pertenecen al grupo. Cuando se unen dos grupos se pueden calcular las nuevas distancias entre ellos sin utilizar los elementos originales. Puede demostrarse que el cuadrado de la distancia euclídea de un grupo $C$ a la unión de los grupos A, con $n_A$ elementos y $B$ con $n_B$ es:\n",
    "$$d^2(C,AB)=\\frac{n_A}{n_A+n_B}d^2(C,A)+\\frac{n_B}{n_A+n_B}d^2(C,B)-\\frac{n_An_B}{(n_A+n_B)^2}d^2(A,B)$$\n",
    "\n",
    "\n",
    "5. **(Ward linkage) Método de Ward.** $d(C,AB)$ es la distancia euclídea al cuadrado entre el vector media muestral de los elementos en ambos clústeres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método divisivo (diana)\n",
    "\n",
    "En análisis de clúster divisivo (diana: divisive analysis), la idea es que en cada paso, las observaciones se dividan en un grupo $A$ y otro grupo $B$.\n",
    "\n",
    "1. El grupo $A$ se inicializa extrayendo la observación que tiene la mayor distancia media hacia las otras observaciones. En un primer paso esa observación es la que conforma el nuevo clúster $A$, y el resto conforman el clúster $B$.\n",
    "\n",
    "\n",
    "2. Dada la separación entre $A$ y $B$, para cada observación en $B$ se calculan las siguientes cantidades:\n",
    "\n",
    "    a) La distancia media entre esa observación concreta de $B$ y las demás observaciones en $B$.\n",
    "    \n",
    "    b) La distancia media de esa observación concreta de $B$ y todas las observaciones de $A$.\n",
    "\n",
    "\n",
    "3. Luego, se calculan las diferencias entre los resultados de (a) y (b) del punto (2) para cada observación de $B$.\n",
    "\n",
    "\n",
    "4. Van a haber dos posibilidades:\n",
    "\n",
    "    i. Si todas las diferencias son negativas, detendremos el algoritmo.\n",
    "    \n",
    "    ii. Si alguna diferencia es positiva, tomaremos la observación de $B$ con la diferencia positiva más grande y la moveremos al clúster $A$ y repetiremos todos los cálculos.\n",
    "    \n",
    "    \n",
    "Este algoritmo proporciona una división binaria, es decir, en clústeres $A$ y $B$, pero luego se puede aplicar dentro de los propios clústeres $A$ y $B$ para dividirlos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de factores\n",
    "\n",
    "El **análisis factorial** tiene por objetivo explicar un conjunto de variables observadas por un pequeño número de variables latentes, o no observadas, que llamaremos factores. \n",
    "\n",
    "Por ejemplo, supongamos que hemos tomado veinte medidas físicas del cuerpo de una persona: estatura, longitud del tronco y de las extremidades, anchura de hombros, peso, etc. Es intuitivo que todas estas medidas no son independientes entre sí, y que conocidas algunas de ellas podemos saber con poco error las restantes porque las dimensiones del cuerpo humano dependen de ciertos factores, y si estos fuesen conocidos podríamos prever con poco error los valores de las variables observadas.\n",
    "\n",
    "El análisis factorial está relacionado con los componentes principales. Sin embargo, una diferencia es que componentes principales es un herramienta descriptiva, mientras que el análisis factorial *presupone un modelo estadístico formal de generación de la muestra dada*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modelo factorial\n",
    "\n",
    "Sea $X=(X_1,X_2,...,X_p)$ nuestra variable multivariante. El modelo de análisis factorial establece que $X$ puede representarse de la siguiente manera:\n",
    " \n",
    "$$X=\\vec{\\mu}+\\Lambda f+U$$ \n",
    "\n",
    "donde:\n",
    "\n",
    "* $f$ es una variable aleatoria multivariante de dimensión $m$ ($m<p$). Este $f$ es llamado *vector de variables latentes* o *vector de factores no observados*. Como no podemos observar los factores $f$, se supone sin perder generalidad que su vector de medias es $\\boldsymbol{0}$, las correlaciones entre sus variables son 0 y las varianzas de cada componente es 1.\n",
    "\n",
    "\n",
    "* $\\Lambda$ es una matriz de $p$ filas y $m$ columnas, donde sus elementos son constantes que desconocemos. Contiene los coeficientes que describen cómo es que $f$ afecta al vector aleatorio original $X$. Se le conoce como *matriz de carga*.\n",
    "\n",
    "\n",
    "* $U$ es una variable aleatoria multivariante con vector de medias $\\boldsymbol{0}$ y matriz de covarianzas $\\psi$, donde $\\psi$ es una matriz diagonal. Se trata de las perturbaciones no observadas (por ejemplo, el error que cometemos cuando usamos instrumentos no calibrados). Recoge el efecto de de todas las variables distintas de los factores que influyen sobre $X$. Supondremos también que $U$ y $f$ están incorrelacionados.\n",
    "\n",
    "\n",
    "* $\\vec{\\mu}$ es el vector de medias (poblacional) de $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamaremos $\\Sigma$ a la matriz de covarianzas de $X$.\n",
    "\n",
    "De esta manera, **estamos diciendo que cada dato de nuestra tabla es el resultado de la media de la característica a la que corresponde ese dato, junto con los factores escondidos que relacionan las características y perturbaciones que no podemos observar**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades\n",
    "\n",
    "Notemos que $X-\\vec{\\mu}=\\Lambda f+U$, de modo que $(X-\\vec{\\mu})f^T=\\Lambda ff^T+Uf^T$. Tomando esperanza a ambos lados, se sigue que $E[(X-\\vec{\\mu})f^T]=\\Lambda$. Pero el lado izquierdo de esta última igualdad es $Cov(X-\\vec{mu},f)$. Por lo tanto $\\Lambda$ no es otra cosa mas que la matriz de covarianzas de $X-\\vec{\\mu}$ con $f$. En concreto: la entrada $\\Lambda_{ij}$ es la covarianza entre la componente $X_i$, de la multivariante $X$, y la componente $f_j$, del multivariante $f$.\n",
    "\n",
    "También se deduce la igualdad fundamental $$\\Sigma=\\Lambda\\Lambda^T+\\psi.$$\n",
    "\n",
    "Esta establece que la matriz de covarianzas de los datos observados admite una descomposición como suma de dos matrices.\n",
    "\n",
    "El primer sumando, la matriz $\\Lambda\\Lambda^T$, contiene la información sobre cómo los factores afectan la covarianza. Esta matriz contiene la parte común al conjunto de las variables y depende de las covarianzas entre las variables y los factores.\n",
    "\n",
    "El segundo sumando, la matriz $\\psi$ (recordemos que es una matriz diagonal), contiene la información sobre las perturbaciones en cada variable.\n",
    "\n",
    "Ahora bien... recordemos que $\\Sigma$ es la matriz de covarianzas de $X$. Los elementos de su diagonal corresponden a las varianzas de cada componente de $X$. Es decir, $\\sigma_i^2$.\n",
    "\n",
    "De $\\Sigma=\\Lambda\\Lambda^T+\\psi$ se sigue que $$\\sigma_i^2=\\sum_{j=1}^m\\Lambda_{ij}^2+\\psi^2_i$$\n",
    "\n",
    "La suma $\\sum_{j=1}^m\\Lambda_{ij}^2$ es llamada *i-ésima comunalidad* y se denota por $h_i^2$. El término $\\psi^2_i$ se conoce como *i-ésima unicidad*.\n",
    "\n",
    "Todo lo anterior se resume como **varianza observada = variabilidad común (comunalidad) + variabilidad específica (unicidad)**.\n",
    "\n",
    "Esto es análogo a la descomposición clásica de la variabilidad de los datos en una parte explicada y otra no explicada que se realiza en el análisis de la varianza.\n",
    "\n",
    "En el modelo factorial la parte explicada es debida a los factores y la no explicada es debido al ruido o componente aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicidad del modelo\n",
    "\n",
    "Observemos que tenemos un problema de indeterminación; a saber, si tenemos matrices de carga $\\Lambda$ y $\\Lambda^*$ y dos vectores de factores $f$ y $f^*$, ¿qué pareja $(\\Lambda,f)$ y $(\\Lambda,f^*)$ nos sirve?\n",
    "\n",
    "Aquí hay un matiz **muy importante**. En el modelo original, supusimos que los factores no están correlacionados entre sí. ¿Será que podemos encontrar una $f$ donde las correlaciones no sean 0 y siga explicando la misma información sobre $X$?\n",
    "\n",
    "La respuesta es sí; y de hecho, cualquier para cualquier conjunto de factores $f^*$ que explique a $X$ siempre existe un conjunto de factores $f$ con variables incorrelacionadas que también explica a $X$; y lo que es mejor: al revés también es cierto (es decir, si las variables de $f$ no tienen correlación, entonces siempre existe un conjunto de factores $f^*$ con variables correlacionadas que explica exactamente la misma información sobre $X$).\n",
    "\n",
    "En resumen, la restricción sobre la incorrelacionalidad de los factores no es en realidad tan fuerte. Supongamos pues, como antes, que la matriz de covarianza de $f$ es la identidad.\n",
    "\n",
    "Ahora bien, es sencillo demostrar que si $H$ es una matriz ortogonal, entonces tomando $\\Lambda^*=\\Lambda H$ y $f^*=H^Tf$, el modelo de factores es exactamente el mismo. Esto significa el modelo factorial está indeterminado ante rotaciones. O sea que no podemos distinguir modelos \"cuando hay matrices ortogonales de por medio\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varimax\n",
    "\n",
    "La interpretación de los factores se facilita si los que afectan a algunas variables no lo hacen a otras y al revés. Este objetivo conduce al criterio de maximizar la varianza de los coeficientes que definen los efectos de cada factor sobre las variables observadas.\n",
    "\n",
    "Bajo este criterio, se selecciona la llamada *matriz de varimax* $H^*$, que es una matriz ortogonal, la cual tiene la propiedad de que si $\\Lambda$ es la matriz de carga, entonces $\\Lambda^*=\\Lambda H^*$ es una matriz de carga que hace que haya factores con correlaciones altas con un número pequeño de variables y correlaciones nulas en el resto, quedando así redistribuida la varianza de los factores..\n",
    "\n",
    "En R, se utiliza la función ``varimax`` para calcularla.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis factorial de componentes principales\n",
    "\n",
    "Si tenemos nuestros datos y sabemos que las variables univariantes tienen diferentes unidades de medidas, es preferible estandarizar las variables. \n",
    "\n",
    "El método consiste en lo siguiente:\n",
    "\n",
    "1) No importa si no conoces la distribución de la variable multivariante $X$.\n",
    "\n",
    "\n",
    "2) Calcular el vector de medias muestral $\\overline{x}$. Si hay diferentes unidades de medidas, utilizar la matriz de correlaciones muestral $R$. En caso contrario, no importa si se toma la de covarianzas $S$ o la de correlaciones.\n",
    "\n",
    "``\n",
    "S=cov(data)\n",
    "R=cor(data)\n",
    "mu=colMeans(data)\n",
    "``\n",
    "\n",
    "\n",
    "3) Para seleccionar $m$, el número de factores, se hace como en PCA: utilizar la varianza explicada por los componentes (por lo tanto debemos calcular valores propios y vectores propios).\n",
    "\n",
    "Si T es la matriz que se calculó previamente (T=S o T=R según sea el caso) entonces\n",
    "\n",
    "``\n",
    "propios_T=eigen(T)\n",
    "propios_valores.T = propios_T$values\n",
    "propios_vectores.T = propios_T$vectors\n",
    "proporcion_variablidad.T = propios_valores.T/sum(propios_valores.T)\n",
    "proporcion_variablidad_acum = cumsum(propios_valores.T)/sum(propios_valores.T)\n",
    "``\n",
    "\n",
    "4) Estimar la matriz de carga $\\Lambda$ usando los valores propios y los vectores propios y aplicarle varimax.\n",
    "\n",
    "``\n",
    "L_estimada01.T = propios_vectores.T[,c(1:m)] %*% diag(sqrt(propios_valores.T[c(1:m)]))\n",
    "L_estimada01.T.varimax = varimax(L_estimada01.T)\n",
    "L_estimada01.T.varimax\n",
    "``\n",
    "\n",
    "5) Estimar $\\psi$, la matriz de covarianzas de los errores:\n",
    "\n",
    "``\n",
    "psi.estimada01.T = diag(diag(T-L_estimada01.T.varimax$loadings %*% t(L_estimada01.T.varimax$loadings)))\n",
    "``\n",
    "\n",
    "6) Obtenemos los scores\n",
    "\n",
    "``\n",
    "scores_estimada01.T  <- scale(data) %*% as.matrix(L_estimada01.T.varimax$loadings)\n",
    "scores_estimada01.T\n",
    "``\n",
    "\n",
    "7) Graficamos los scores\n",
    "\n",
    "``\n",
    "plot(scores_estimada01.T[,1],scores_estimada01.T[,2],xlab=\"1er factor\",ylab=\"2do factor\",\n",
    "main=\"Scores de 1er factor vs 2do factor por PCFA\",pch=19,col=\"blue\")\n",
    "text(scores_estimada01.T[,1],scores_estimada01.T[,2],labels=ETIQUETAS DE LOS PUNTOS,pos=4,col=\"blue\")\n",
    "``\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis factorial por máxima verosimilitud\n",
    "\n",
    "Este método es el habitualmente preferido por los estadísticos. Si sabemos que $X$ es normal  multivariante, o se parece mucho a una normal multivariante (por ejemplo, si es elíptica),  se ocupa una cierta \"función de distancia\". \n",
    "\n",
    "Las estimaciones de los pesos factoriales se obtienen minimizando esta función, y esto es equivalente a maximizar la función de verosimilitud del modelo factorial asumiendo normalidad.\n",
    "\n",
    "Una ventaja del método de máxima verosimilitud es que lleva asociado un test estadístico para estimar el número de factores.\n",
    "\n",
    "Para realizarlo en R utilizamos la función ``factanal``\n",
    "\n",
    "**Observaciones.** La función ``factanal`` siempre escala los datos originales. \n",
    "\n",
    "1) Aplicamos la función tantas veces como sea necesario nuestro valor de significación $\\alpha$.\n",
    "\n",
    "``analisis_factores=factanal(data,factors=1,rotation=\"varimax\",scores=\"regression\")\n",
    "analisis_factores``\n",
    "\n",
    "Si el $p$-valor obtenido es menor que $\\alpha$, cambiamos factors a 2 y así sucesivamente hasta que el $p$-valor sea mayor que $\\alpha$.\n",
    "\n",
    "2) Calculamos la matriz de carga:\n",
    "\n",
    "``L_estimada02.T.varimax = analisis_factores$loadings\n",
    "L_estimada02.T.varimax``\n",
    "\n",
    "3) Estimamos la matriz de covarianza de los errores\n",
    "\n",
    "``\n",
    "psi.estimada02.T = diag(analisis_factores$uniquenesses)\n",
    "psi.estimada02.T\n",
    "``\n",
    "\n",
    "4) Hacemos los gráficos.\n",
    "\n",
    "``\n",
    "plot(analisis_factores$scores[,1],analisis_factores$scores[,2],xlab=\"1er factor\",ylab=\"2do factor\",\n",
    "main=\"Scores de 1er factor vs 2do factor con MLE\",pch=19,col=\"blue\")\n",
    "text(analisis_factores$scores[,1],analisis_factores$scores[,2],labels=ETIQUETAS DE LOS PUNTOS,pos=4,col=\"blue\")\n",
    "``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
